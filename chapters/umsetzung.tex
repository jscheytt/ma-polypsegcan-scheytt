\chapter{Umsetzung}

Dieses Kapitel beschreibt die Umsetzung der gewählten Methodik mit den zur Verfügung stehenden Materialien.
Dazu wird auf das gewählte Framework, die Vorverarbeitung des Datasets und die durchgeführten Experimente eingegangen.



\section{TensorFlow}

Zur Entwicklung des \gls{can} wurde das Deep-Learning-Framework \emph{TensorFlow}~\cite{Abadi.2016} gewählt.
Zugrunde liegt TensorFlow das Paradigma, dass das Modell zuerst als Graph mit Entitäten und Operationen definiert und dann schrittweise trainiert wird.
Dies macht zwar die Programmierung etwas weniger intuitiv, sorgt aber dafür, dass viele Operationen im Voraus vereinfacht und teilweise auch parallelisiert werden können, sodass das Training insgesamt schneller abläuft.

\citeauthor{Isola.2017} stellen den originalen Code ihrer Publikation, der in Torch geschrieben ist, online zur Verfügung\footnote{\url{https://github.com/phillipi/pix2pix}}.
Mehrere Portierungen auf andere Frameworks wurden bereits geschrieben, darunter auch mehrere für TensorFlow.
Gewählt wurde in dieser Arbeit die TensorFlow-Portierung von Christopher Hesse\footnote{\url{https://github.com/affinelayer/pix2pix-tensorflow}}, da diese von allen am besten dokumentiert ist.



\section{Vorverarbeitung}

Intern nimmt der Code die Bilddaten als Kombination aus Input und Target an.
Hierbei muss für jedes Input-Target-Paar ein Bild generiert werden, welches das Target rechts vom Input platziert, sodass ein zusammengesetztes Bild mit doppelter Breite entsteht.
Ein Datensatz aus CVC-ColonDB (s.~\autoref{tab:datasets}) hat dann bspw. die Auflösung 1000$\times$574.

Das \gls{can} kann, wie viele Deep-Learning-Architekturen, die auf Bilddaten arbeiten, nativ nur mit quadratischen Bildern umgehen, auch wenn theoretisch jede Auflösung verarbeitet werden kann, indem man eine zusätzliche Faltung beim Input und Output verwendet.
Trotzdem werden der Einfachheit wegen beim Training nur quadratische Bilder erlaubt.
Viele der Datensätze haben zwar schwarze Ränder, die keine Information enthalten, und nach deren Entfernen man ein ungefähr quadratisches Bild erhalten würde.
Allerdings sind die relevanten Bildausschnitte je nach Bildsequenz anders im Gesamtbild positioniert.
Um dieser Varianz gerecht zu werden und gleichzeitig möglichst keine Bildinformation zu verlieren, wurde die längere Bildseite als Seitenlänge des neuen quadratischen Bildes gewählt und der Rest des Bildes mit Nullen, also Schwarz, aufgefüllt.

Im originalen Code sind außerdem nur die Dateiformate \gls{jpg} und \gls{png} erlaubt.
Das Dataset enthält allerdings eine Mischung aus \gls{bmp} und \gls{tiff} (s.~\autoref{tab:dataformats}).
TensorFlow bietet native Methoden, um mit dem BMP-Format umzugehen, also wurde der Code erweitert, sodass \gls{bmp} als Eingabeformat für Datensätze möglich wird.
Für \gls{tiff} bietet TensorFlow allerdings keine Verarbeitungsmöglichkeiten.
Da die Targets Binärbilder sind und deshalb nicht mit verlustbehafteten Kompressionsverfahren wie bei \gls{jpg} behandelt werden sollten, wurde folgendes Verfahren gewählt:
Alle Input-Bilder wurden zu \gls{bmp} konvertiert und alle Target-Bilder wurden zu \gls{png} konvertiert, sofern sie jeweils noch nicht in diesem Format vorlagen.
Die kombinierten Input-Target-Bilder wurden dann als \gls{png} produziert.

\begin{table}
	\centering
	\caption{Dateiformate der Datasets}
	\label{tab:dataformats}
	\begin{tabular}{ccc} 
		\toprule
		Sub-Dataset  & Input                       & Target                       \\ 
		\midrule
		CVC-ColonDB  & \gls{bmp}  & \gls{bmp}   \\
		CVC-PolypHD  & \gls{bmp}  & \gls{tiff}  \\
		CVC-ClinicDB & \gls{tiff} & \gls{tiff}  \\
		\bottomrule
	\end{tabular}
\end{table}



\subsection{Aufteilung der Splits}

Für ein Training von maschinellen Lernverfahren ist es üblich, das Dataset in mehrere Untergruppen aufzuteilen.
Meistens werden die Daten aufgeteilt in Sets für das Training, die Validierung und das Testen~\cite{Guyon.1997}.
Die Daten des Trainings-Split werden dem Netz direkt zur Verfügung gestellt, um Gradienten zu berechnen und daraus Parameter-Updates abzuleiten.
In regelmäßigen Abständen wird das Netz auf den Daten des Validierungs-Splits ausgeführt, um sicherzustellen, dass das Netz auch auf nicht gesehenen Daten gut vorhersagt und kein Overfitting stattfindet.
Am Ende des Trainings wird der Algorithmus auf dem Test-Split evaluiert, der in der Regel größer als der Validierungs-Split ist.

Das Größenverhältnis dieser Splits zueinander lässt sich zwar rechnerisch bestimmen~\cite{Guyon.1998,Guyon.1997}, allerdings sind die Verfahren ziemlich komplex.
Oftmals werden Größenverhältnisse zwischen Training- und Test-Set von 75:25 bis 90:10 vorgeschlagen.
Im Rahmen der \gls{giana} Challenge ist die Einteilung in Training- und Test-Set bereits vorgegeben.
Bezogen auf die Anzahl der \emph{Samples} macht der Test-Split 63~\% des Datasets aus, was ungewöhnlich groß ist.
Allerdings enthält der Test-Split nur 25~\% aller \emph{Pixel} des Datasets, da das Training-Set mit CVC-Polyp-HD ein sehr hochauflösendes Sub-Dataset enthält.

In dieser Arbeit wurde die Aufteilung des Training-Sets in die Splits für Training und Validierung folgendermaßen gewählt:
Sowohl von CVC-ColonDB als auch von CVC-PolypHD wurden Sub-Splits im Verhältnis 75:25 durch zufälliges Sampling erstellt.
Die beiden größeren Sub-Splits der Sub-Datasets wurden zum Trainings-Split kombiniert, die kleiner Sub-Splits zum Validierungs-Split.
Dies sollte eine möglichst gleichmäßige Verteilung der beiden Sub-Datasets über die beiden Splits ermöglichen.
Wünschenswert wäre eine Einteilung der beiden Splits nach \emph{Patienten} gewesen wie bei \citeauthor{Vazquez.2017}~\cite{Vazquez.2017}, aber die Zuordnung von Datensätzen zu Patienten ist weder öffentlich zugänglich noch direkt aus den Daten erkenntlich.



\section{Experimente}

Viele Hyperparameter des \gls{can} sind im bereitgestellten Code bereits mit Standardwerten belegt (s~\autoref{tab:train_def_val}).
Einige Datasets, die \citeauthor{Isola.2017} in ihrem Paper beschreiben, stellen sie öffentlich zur Verfügung.
Trainiert man das Netz auf einigen dieser Datasets ohne die Hyperparameter zu verändern, konvergiert das Netz bereits nach einigen Epochen.
Deshalb wird anfangs ein Trainingslauf auf dem \gls{giana} Dataset mit den vorgegebenen Standardwerten des Netzes durchgeführt.

\begin{table}
	\centering
	\caption{Standardwerte der Trainingsparameter}
	\label{tab:train_def_val}
	\begin{tabular}{rl} 
		\toprule
		Parameter                                            & Standardwert  \\ 
		\midrule
		Batch-Größe                                          & 1             \\
		Anzahl der Filter in der ersten Generator-Schicht    & 64            \\
		Anzahl der Filter in der ersten Diskriminator-Schicht    & 64            \\
		Lernrate des Optimierungsalgorithmus                 & $ 2 \times 10^{-4} $    \\
		Momentum-Term des~Optimierungsalgorithmus            & 0.5           \\
		$ \lambda $ (Gewichtung des L1-Terms der Verlustfunktion) & 100           \\
		\bottomrule
	\end{tabular}
\end{table}

Anschließend wird untersucht, wie sich die Veränderung bestimmter Parameter auf die Ergebnisse auswirkt.
Dazu wird zum einen mit verschiedenen Batch-Größen und zum anderen mit \emph{Dataset Augmentation} experimentiert.
Die Batch-Größe wird mit wachsenden Zweierpotenzen, nämlich von 1 bis 64, und mit mehreren Trainingsläufen pro Batch-Größe ausprobiert.

Dataset Augmentation umfasst verschiedene Techniken, um die Größe und Varianz eines Datasets durch Transformationen zu vergrößern~\cite{Goodfellow.2016}.
Dazu können geometrische Transformationen wie Translation, Rotation und Warping gehören, aber auch andere Transformationen wie das Hinzufügen von zufälligem Rauschen oder Salt-and-Pepper-Rauschen.
Durch solche Transformationen kann man zum einen ein Dataset künstlich vergrößern, indem man einzelne Datensätze in transformierter Form kopiert und dem Dataset hinzufügt.
Andererseits können Netze beim Training besonders von erhöhter Varianz profitieren, vorausgesetzt geometrische Transformationen werden am Target in exakt derselben Weise durchgeführt.

\citeauthor{Vazquez.2017}~\cite{Vazquez.2017} nutzen ein FCN-8s für eine Polypensegmentierung und setzen dafür verschiedene Augmentierungen ein, die dazu gedacht sind, die Varianz angemessen zu erhöhen, ohne das Polypenaussehen zu drastisch zu verändern.
Eine zu drastische Veränderung des Bildmaterials könnte zu unrealistischen Verformungen führen, sodass das zugehörige Target eines Inputs möglicherweise nicht mehr wahrheitsgemäß wäre.
Die Transformationen von \citeauthor{Vazquez.2017} beinhalten die folgenden zufällig angewandten Operationen:

\begin{itemize}
	\item Zoom mit einem Faktor von 0,9 bis 1,1
	\item Rotation bis zu 180~\textdegree
	\item Scherung mit einem Faktor von 0 bis 0,4
	\item Warping mit $ \sigma $ von 0 bis 10
\end{itemize}

Die Autoren stellen auf ihrem Dataset bei einer Kombination all dieser Transformationen eine Verbesserung der Polypensegmentierung um ca. 10 Prozentpunkte fest.
Die dort angewandten Transformationen kommen auch in dieser Arbeit zum Einsatz.
