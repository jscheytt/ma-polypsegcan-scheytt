\chapter{Umsetzung}

Dieses Kapitel beschreibt die Umsetzung der gewählten Methodik mit den zur Verfügung stehenden Materialien.
Dazu wird auf das gewählte Framework, die Vorverarbeitung des Datasets und die durchgeführten Experimente eingegangen.



\section{TensorFlow}

Zur Entwicklung des \gls{can} wurde das Deep-Learning-Framework \emph{TensorFlow}~\cite{Abadi.2016} gewählt.
TensorFlow liegt ein Paradigma zugrunde, nach dem das Modell zuerst als Graph mit Entitäten und Operationen definiert und erst dann schrittweise trainiert wird.
Im Gegensatz zu imperativen Frameworks wie PyTorch ist dadurch zwar insbesondere das Debugging erschwert, aber diese gewählte Architektur sorgt dafür, dass viele Rechenoperationen im Voraus vereinfacht und teilweise auch parallelisiert werden können, sodass das Training insgesamt schneller abläuft~\cite{Goodfellow.2016}.

\citeauthor{Isola.2017} stellen den originalen Code ihrer Publikation, der in Torch geschrieben ist, online zur Verfügung\footnote{\url{https://github.com/phillipi/pix2pix}}.
Portierungen auf andere Python-basierte Frameworks wurden bereits geschrieben, darunter auch mehrere für TensorFlow.
Gewählt wurde in dieser Arbeit die TensorFlow-Portierung von Christopher Hesse\footnote{\url{https://github.com/affinelayer/pix2pix-tensorflow}}, da diese von allen am besten dokumentiert ist.

Diese Portierung konnte nur mit der TensorFlow-Version 1.5.0 nutzbar gemacht werden, da sie seit etwa 8 Monaten nicht mehr aktiv weiterentwickelt wurde.
Dadurch stehen die Kernfunktionalitäten von TensorFlow zur Verfügung, besonders praktische Weiterentwicklungen wie die Eager Execution zur unmittelbaren Evaluierung von Graph-Komponenten aber nicht.
Da kein GPU zur Verfügung stand, wurde die CPU-basierte Variante von TensorFlow genutzt.



\section{Vorverarbeitung}

Intern nimmt der Code die Bilddaten als Kombination aus Input und Target an.
Hierbei muss für jedes Input-Target-Paar ein Bild generiert werden, welches das Target rechts vom Input platziert, sodass ein zusammengesetztes Bild mit doppelter Breite entsteht.
Ein Datensatz aus CVC-300 (s.~\autoref{tab:datasets}) hat dann bspw. die Auflösung 1000$\times$574.

Im originalen Code sind außerdem nur die Dateiformate \gls{jpg} und \gls{png} erlaubt.
Das Dataset enthält allerdings eine Mischung aus \gls{bmp} und \gls{tiff} (s.~\autoref{tab:dataformats}).
TensorFlow bietet native Methoden, um mit dem BMP-Format umzugehen, also wurde der Code erweitert, sodass \gls{bmp} als Eingabeformat für Datensätze möglich wird.
Für \gls{tiff} bietet TensorFlow allerdings keine Verarbeitungsmöglichkeiten.
Da die Targets Binärbilder sind und deshalb nicht mit verlustbehafteten Kompressionsverfahren wie bei \gls{jpg} behandelt werden sollten, wurde folgendes Verfahren gewählt:
Alle Input-Bilder wurden zu \gls{bmp} konvertiert und alle Target-Bilder wurden zu \gls{png} konvertiert, sofern sie jeweils noch nicht in diesem Format vorlagen.
Die kombinierten Input-Target-Bilder wurden dann als \gls{png} produziert.

\begin{table}
	\centering
	\caption{Dateiformate der Datasets}
	\label{tab:dataformats}
	\begin{tabular}{ccc} 
		\toprule
		Sub-Dataset & Input & Target \\ 
		\midrule
		CVC-300 & \gls{bmp} & \gls{bmp} mit nur 1 Kanal \\
		CVC-612 & \gls{tiff} & \gls{tiff} \\
		CVC-HDTrain & \gls{bmp} & \gls{tiff} \\
		CVC-HDTest & \gls{bmp} & \gls{tiff} \\
		\bottomrule
	\end{tabular}
\end{table}



\subsection{Aufteilung der Splits}

Für ein Training von maschinellen Lernverfahren ist es üblich, das Dataset in mehrere Untergruppen aufzuteilen.
Meistens werden die Daten aufgeteilt in Sets für das Training, die Validierung und das Testen~\cite{Guyon.1997}.
Die Daten des Trainings-Split werden dem Netz direkt zur Verfügung gestellt, um Gradienten zu berechnen und daraus Parameter-Updates abzuleiten.
In regelmäßigen Abständen wird das Netz auf den Daten des Validierungs-Splits ausgeführt, um sicherzustellen, dass das Netz auch auf nicht gesehenen Daten gut vorhersagt und kein Overfitting stattfindet.
Am Ende des Trainings wird der Algorithmus auf dem Test-Split evaluiert, der in der Regel größer als der Validierungs-Split ist.

Das Größenverhältnis dieser Splits zueinander lässt sich zwar rechnerisch bestimmen~\cite{Guyon.1998,Guyon.1997}, allerdings sind die Verfahren ziemlich komplex.
Oftmals werden Größenverhältnisse zwischen Training- und Test-Set von 75:25 bis 90:10 vorgeschlagen.
Im Rahmen der \gls{giana} Challenge ist die Einteilung in Training- und Test-Set bereits vorgegeben.
Bezogen auf die Anzahl der \emph{Samples} macht der Test-Split beim SD-Dataset 67~\% und beim HD-Dataset 66~\% aus, was die üblichen Größenverhältnisse beinahe umkehrt.
Betrachtet man allerdings die Anzahl der Pixel, kommt zumindest beim SD-Dataset der Test-Split auf nur 44~\% des Datasets, da dessen Auflösung geringer ist.

In dieser Arbeit wurde die Aufteilung des Training-Sets in die Splits für Training und Validierung pro Dataset im Verhältnis 80:20 gewählt.
Wie später beschrieben, wird in dieser Arbeit nur das SD-Dataset verwendet.
Das gewählte Verhältnis ist ein Kompromiss zwischen den Zielen, so viel wie möglich der Trainingsdaten beim Lernen zu nutzen und gleichzeitig so viel Varianz wie möglich im Validierungsset abzubilden.
Konkret entspricht der Anteil des Validierungssets beinahe der fünffachen Menge der Sequenzen, wodurch bei zufälligem Sampling des Validierungsset optimalerweise von jeder Sequenz mindestens ein Datensatz enthalten ist.
Wünschenswert wäre eine Einteilung dieser beiden Splits nach \emph{Patienten} gewesen wie bei \citeauthor{Vazquez.2017}~\cite{Vazquez.2017}, aber die Zuordnung von Datensätzen zu Patienten ist weder öffentlich zugänglich noch direkt aus den Daten ersichtlich.



\subsection{Auflösung der Bilddaten}

Das \gls{can} kann, wie viele Deep-Learning-Architekturen, die auf Bilddaten arbeiten, nativ nur mit quadratischen Bildern umgehen, konkret mit einer Seitenlänge von 256 Pixeln.
Sowohl das Einlesen der Datensätze als auch die Ausgabe des Generators bereitet in diesem Fall Probleme, da keines der Bilder in den Datasets die passende Auflösung noch das richtige Seitenverhältnis hat.



\subsubsection{Einlesen}

Viele der Datensätze haben zwar schwarze Ränder, die keine Information enthalten, und nach deren Entfernen man ein ungefähr quadratisches Bild erhalten würde.
Allerdings sind die relevanten Bildausschnitte je nach Bildsequenz anders im Gesamtbild positioniert.
Um dieser Varianz gerecht zu werden und gleichzeitig möglichst keine Bildinformation zu verlieren, wurde die längere Bildseite als Seitenlänge des neuen quadratischen Bildes gewählt, der originale Bildinhalt horizontal und vertikal zentriert und der Rest des Bildes mit Nullen, also Schwarz, aufgefüllt.

Zwar hat nun immer noch keiner der Datensätze die Zielauflösung, dennoch ist ein Einsatz der \emph{Trainingsdaten} auf verschiedene Arten vorstellbar:

\begin{enumerate}
	\item Herunterskalieren der Inputs und Targets
	\item Zufälliges Samplen von 256$\times$256-Patches von Inputs und Targets
\end{enumerate}

Die erste Variante hat bei der vorliegenden Problemstellung den Nachteil, dass es je nach Skalierungsmethodik zu Aliasing-Artefakten kommen kann.
Dies wirkt sich besonders nachteilig beim Target aus, da es sich um ein Binärbild handelt, dass dann de facto zum Graustufenbild umgewandelt wird.
Bilineare und bikubische Skalierungsverfahren beziehen eine 2$\times$2- bzw. eine 4$\times$4-Nachbarschaft um einen zu interpolierenden Punkt mit ein und liefern damit sanftere Übergänge, während das Nearest-Neighbor-Verfahren naiv den Wert des naheliegendsten Nachbars kopiert.
Damit scheint letzteres Verfahren tendenziell besser geeignet, weil die binäre Natur des Targets beibehalten würde, allerdings können sich besonders an kontrastreichen Kanten treppenartige Artefakte bilden.

Letztere Variante umgeht diese Artefaktprobleme, da die originale Auflösung beibehalten wird und immer ein Teilausschnitt des Inputs und Targets an jeweils derselben Position gewählt wird.
Würde ein solcher Patch im Target allerdings gar keinen Vordergrundpixel enthalten, könnte der Generator nur sehr wenig von diesem Sample lernen.
Dementsprechend sollte garantiert sein, dass ein gewisser Prozentsatz der Pixel des gesampleten Patches zur Polypenklasse gehören.
Entweder garantiert man dies durch eine Einschränkung der Samplingregion bspw. auf ungefähr die Bounding Box des Polypen vor dem Sampling oder durch wiederholtes Sampling bis ein gewisser Prozentsatz an Vordergrundpixeln erreicht ist.
Ein wiederholtes Sampling aber könnte die Performance des Trainings verschlechtern.
Außerdem müsste zusätzlicher Aufwand an Evalution investiert werden, um herauszufinden, welcher Prozentsatz an Vordergrundpixeln ungefähr genug ist.

Aus diesen Gründen wird der Ansatz mit Herunterskalieren gewählt.
Dabei ist die Frage noch nicht geklärt, ob sich das Training stärker verschlechtert, wenn das Netz lernt, dass Graustufen- statt Binärbildern gewollt sind, oder wenn es lernt, dass treppenartige Objektgrenzen realistisch sind.
TensorFlow bietet zur Skalierung verschiedene Interpolationsverfahren an: Area-, Nearest-Neighbor-, bilineare und bikubische Interpolation.
Für eine Herunterskalierung eignet sich die Area-Interpolation am besten, da sie als einzige dieser Optionen Aliasing-Effekte beim Downscaling vermeidet.



\subsubsection{Ausgabe von höheren Auflösungen zur Testzeit}

\citeauthor{Isola.2017} schreiben, dass der Generator durch Faltung auch zur Erzeugung von beliebig großen Outputs zur Testzeit genutzt werden kann~\cite{Isola.2017}.
Sie demonstrieren dies anhand von Bildern mit einer Auflösung von 512$\times$512.
Leider erklären sie nicht genauer, wie eine solche Faltung ohne einen weiteren Lernschritt möglich sein soll, denn für jede hochskalierende Faltung müssen zuerst Gewichte gelernt werden.
Es könnte sich der Verdacht aufdrängen, dass statt einer tatsächlichen Faltungsoperation einfach nur 4 sich nicht überlappende 256$\times$256-Patches des 512$\times$512-Bildes dem Generator übergeben wurden; auf direkte Nachfrage bei den Autoren erfolgte dazu leider keine Antwort.

Wenn der Generator eine Ausgabe erzeugen soll, die mit der Auflösung des originalen Inputs übereinstimmt, ohne dass vorher eine weitere Hochskalierungsschicht trainiert wird, sind folgende Herangehensweisen denkbar:

\begin{enumerate}
	\item Hochskalieren des Outputs
	\item Einlesen, Ausgeben und Zusammensetzen von Patches
\end{enumerate}

Auch in diesem Fall führt der erste Weg dazu, das Aliasing-Artefakte auftreten, die aus dem (zumindest potenziellen) Binärbild ein Graustufenbild erzeugen, wenn man statt Nearest-Neighbor-Verfahren ein bilineares oder bikubisches Verfahren wählt.
Dieselben Gefahren hinsichtlich treppenartiger Artefakte wie beim Einlesen gelten auch hier.

Alternativ könnte man den Input in 256$\times$256-Patches aufteilen, von jedem Patch eine Ausgabe erzeugen lassen und diese Output-Patches wieder zu einem Gesamt-Output zusammensetzen.
Bei Pixeln, die in überlappenden Regionen des Gesamt-Outputs liegen, müsste dann ein Mittelwert aller Überlappungen an diesem Bildpunkt berechnet werden.
Aliasing-Artefakte könnte man an solchen Stellen durch eine Schwellwertberechnung entgegentreten.

Da sich die Umsetzung dieser zweiten Variante in TensorFlow als sehr kompliziert erwies, wurde stattdessen die Hochskalierung gewählt.
In diesem Fall wird evaluiert, welches der von TensorFlow angeboteten Verfahren bei der Hochskalierung die besten Ergebnisse liefert.

Eine solche Hochskalierung für CVC-612 anzuwenden erscheint sinnvoll, da die hochskalierte Fläche nicht einmal doppelt so groß ist wie die native Ausgabe des Generators.
Bei CVC-HDTest hingegen wäre die hochskalierte Fläche mehr als dreißigmal so groß wie die Ausgabegröße des Generators.
Die Ergebnisse einer solchen Hochskalierung wären sehr wahrscheinlich nicht sehr gut; hier müsste eine patchbasierte Lösung oder eine gelernte Hochskalierung zum Einsatz kommen.
Aus diesem Grund wird in dieser Arbeit \emph{nur das SD-Dataset} im Training verwendet und evaluiert.



\paragraph{Binarisierung}

Da die \glspl{can} darauf ausgelegt sind, Bilder von natürlichen Szenen zu produzieren, ist die Ausgabe des Generators immer ein Farbbild mit drei Kanälen.
Die Zielsetzung, dass das \gls{can} ein Binärbild produziert, kann demnach auf zwei Arten scheitern:

\begin{enumerate}
	\item Das Netz lernt es Graustufenbilder statt Binärbildern zu produzieren.
	\item Das Netz lernt es zwar Binärbilder zu produzieren, aber nur kanalweise; die Kanäle eines Output-Bildes unterscheiden sich.
\end{enumerate}

Notwendigerweise muss mit beiden Problematiken umgegangen werden.
Am sinnvollsten erscheint es, die Schwellwertoperation erst so spät wie möglich durchzuführen.
Darum wird zuerst der Mittelwert aller drei Kanäle gebildet, indem die elementweise Summe der Kanäle durch die Anzahl der Kanäle geteilt wird.
Anschließend wird binarisiert; als Schwellwert wird die Hälfte des Maximalwerts gewählt, da die Ergebnisse sich bereits nahe an Binärbildern befinden.

Der gesamte Ablauf der Nachbearbeitung des Generator-Outputs setzt sich nun folgendermaßen zusammen:

\begin{enumerate}
	\item Hochskalieren
	\item Kanäle zusammenfassen
	\item Binarisieren
\end{enumerate}



\section{Experimente}

Viele Hyperparameter des \gls{can} sind im bereitgestellten Code bereits mit Standardwerten belegt (s~\autoref{tab:train_def_val}).
Einige Datasets, die \citeauthor{Isola.2017} in ihrem Paper beschreiben, stellen sie öffentlich zur Verfügung.
Trainiert man das Netz auf einigen dieser Datasets ohne die Hyperparameter zu verändern, konvergiert das Netz bereits nach einigen Epochen.
Deshalb wird anfangs ein Trainingslauf auf dem \gls{giana} Dataset mit den vorgegebenen Standardwerten des Netzes durchgeführt.

\begin{table}
	\centering
	\caption{Standardwerte der Trainingsparameter}
	\label{tab:train_def_val}
	\begin{tabular}{rl} 
		\toprule
		Parameter & Standardwert \\ 
		\midrule
		Batch-Größe & 1 \\
		Anzahl der Filter in der ersten Generator-Schicht & 64 \\
		Anzahl der Filter in der ersten Diskriminator-Schicht & 64 \\
		Lernrate des Optimierungsalgorithmus & $ 2 \times 10^{-4} $ \\
		Momentum-Term des~Optimierungsalgorithmus & 0.5 \\
		$ \lambda $ (Gewichtung des L1-Terms der Verlustfunktion) & 100 \\
		\bottomrule
	\end{tabular}
\end{table}

Anschließend wird untersucht, wie sich die Veränderung bestimmter Parameter auf die Ergebnisse auswirkt.
Dazu wird zum einen mit verschiedenen Batch-Größen und zum anderen mit \emph{Dataset Augmentation} experimentiert.
Die Batch-Größe wird mit wachsenden Zweierpotenzen, nämlich von 1 bis 64, und mit mehreren Trainingsläufen pro Batch-Größe ausprobiert.

Dataset Augmentation umfasst verschiedene Techniken, um die Größe und Varianz eines Datasets durch Transformationen zu vergrößern~\cite{Goodfellow.2016}.
Dazu können geometrische Transformationen wie Translation, Rotation und Warping gehören, aber auch andere Transformationen wie das Hinzufügen von zufälligem Rauschen oder Salt-and-Pepper-Rauschen.
Durch solche Transformationen kann man zum einen ein Dataset künstlich vergrößern, indem man einzelne Datensätze in transformierter Form kopiert und dem Dataset hinzufügt.
Andererseits können Netze beim Training besonders von erhöhter Varianz profitieren, vorausgesetzt geometrische Transformationen werden am Target in exakt derselben Weise durchgeführt.

\citeauthor{Vazquez.2017}~\cite{Vazquez.2017} nutzen ein FCN-8s für eine Polypensegmentierung und setzen dafür verschiedene Augmentierungen ein, die dazu gedacht sind, die Varianz angemessen zu erhöhen, ohne das Polypenaussehen zu drastisch zu verändern.
Eine zu drastische Veränderung des Bildmaterials könnte zu unrealistischen Verformungen führen, sodass das zugehörige Target eines Inputs möglicherweise nicht mehr wahrheitsgemäß wäre.
Die Transformationen von \citeauthor{Vazquez.2017} beinhalten die folgenden Operationen, die mit einer Wahrscheinlichkeit von 50~\% angewandt werden:

\begin{itemize}
	\item Zoom mit einem Faktor von 0,9 bis 1,1
	\item Rotation von 0 bis 180\textdegree
	\item Scherung mit einem Winkel von 0 bis 0,4
	\item Warping mit $ \sigma $ von 0 bis 10
\end{itemize}

Die Autoren stellen auf ihrem Dataset bei einer Kombination all dieser Transformationen eine Verbesserung der Polypensegmentierung um ca. 10 Prozentpunkte fest.
In dieser Arbeit werden alle genannten Transformationen angewandt bis auf das Warping, da diese nicht genau genug dokumentiert ist, um sie verlässlich zu reproduzieren.
Nach der Durchführung einer Augmentierung wird das erzeugte Bild mit der Hälfte des maximalen Wertes als Schwellwert binarisiert, da bei der Anwendung der Transformationen Interpolationsartefakte auftreten können, die dem Bild Graustufenverläufe hinzufügen.



\subsubsection{Modifikationen der Architektur}

Eine wichtige Veränderung in der Funktionsweise der \glspl{can}, um sie für binäre Segmentierungen gangbar zu machen, ist die \emph{Deaktivierung von Dropout zur Testzeit}.
Dropout beim Training einzusetzen hilft dem Netz, dass jedes einzelne Hidden Unit besser regularisiert.
Diese Methodik zur Testzeit einzusetzen ergibt bei den originalen \glspl{can} Sinn, weil die zusätzliche Stochastizität bei der Erzeugung von natürlichen Szenen durchaus hilfreich sein kann.
Auch wenn diese Stochastizität laut den Autoren gering ist~\cite{Isola.2017}, führt sie bei dieser Problemstellung zu einem hohen Maß an Varianz in den Ergebnissen.
Dem Output einer binären Segmentierung tut eine größere Menge Determinismus gut, deshalb wird Dropout zur Testzeit deaktiviert.
