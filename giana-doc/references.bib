% This file was created with Citavi 6.1.0.0

@incollection{Goodfellow.2014,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 title = {{Generative Adversarial Nets}},
 url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
 pages = {2672--2680},
 bookpagination = {page},
 publisher = {{Curran Associates, Inc}},
 editor = {{Z. Ghahramani} and {M. Welling} and {C. Cortes} and {N. D. Lawrence} and {K. Q. Weinberger}},
 booktitle = {{Advances in Neural Information Processing Systems 27}},
 year = {2014},
 abstract = {}
}


@inproceedings{Isola.2017,
 author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
 title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
 pages = {5967--5976},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5386-0457-1},
 booktitle = {{30th IEEE Conference on Computer Vision and Pattern Recognition}},
 year = {2017},
 abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds oftwitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.},
 doi = {10.1109/CVPR.2017.632},
 file = {08100115:C\:\\Users\\Josia\\Documents\\Citavi 6\\Projects\\Masterthesis\\Citavi Attachments\\08100115.pdf:pdf},
 editora = {Chellappa, Rama and Zhang, Zhengyou and Hoogs, Anthony},
 booksubtitle = {CVPR 2017 : 21-26 July 2016, Honolulu, Hawaii : proceedings},
 location = {Piscataway, NJ},
 eventtitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 venue = {Honolulu, HI},
 organization = {{IEEE Conference on Computer Vision and Pattern Recognition} and {IEEE/CVF Conference on Computer Vision and Pattern Recognition} and CVPR}
}


@misc{Radford.2016,
 author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
 year = {2016},
 title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
 url = {http://arxiv.org/pdf/1511.06434v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning},
 abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
 file = {Radford, Metz et al. 2016 - Unsupervised Representation Learning with Deep:C\:\\Users\\Josia\\Documents\\Citavi 6\\Projects\\Masterthesis\\Citavi Attachments\\Radford, Metz et al. 2016 - Unsupervised Representation Learning with Deep.pdf:pdf},
 note = {Under review as a conference paper at ICLR 2016}
}


@incollection{Ronneberger.2015,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
 pages = {234--241},
 bookpagination = {page},
 volume = {9351},
 publisher = {Springer},
 isbn = {978-3-319-24573-7},
 series = {Lecture notes in computer science Image processing, computer vision, pattern recognition, and graphics},
 editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
 booktitle = {{Medical image computing and computer-assisted intervention - MICCAI 2015}},
 year = {2015},
 abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
 doi = {10.1007/978-3-319-24574-4_28},
 location = {Cham},
 booksubtitle = {18th international conference, Munich, Germany, October 5-9, 2015; proceedings},
 number = {9351},
 file = {10.1007%2F978-3-319-24574-4_28:C\:\\Users\\Josia\\Documents\\Citavi 6\\Projects\\Masterthesis\\Citavi Attachments\\10.1007%2F978-3-319-24574-4_28.pdf:pdf}
}


