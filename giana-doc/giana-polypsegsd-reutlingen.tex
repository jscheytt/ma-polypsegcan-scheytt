\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[
	backend=biber,
	minbibnames=3,
	maxbibnames=3,
	giveninits=true,
]{biblatex}
\addbibresource{references.bib}

\title{GIANA 2018 Polyp Segmentation SD -- Conditional Adversarial Networks}
\author{Josia Scheytt\thanks{Reutlingen University}, Cristobal Curio\footnotemark[1], Oliver Burgert\footnotemark[1]}

\begin{document}

\maketitle

\begin{abstract}
Automatically localizing polyps in colonoscopic imagery is a difficult task because polyp appearance varies greatly in color, texture, and shape.
Recently conditional variants of GANs have been developed for a wide range of image-to-image translation tasks.
To our knowledge, we are the first to use this deep learning architecture for segmenting colorectal polyps.
We extend the original methodology for the binary segmentation task.
\end{abstract}

Generative Adversarial Networks (GANs)~\cite{Goodfellow.2014} are designed to unsupervisedly learn to generate realistic images in terms of closeness to the training data.
Conditioned GANs receive an additional input for conditioning the output which makes them useful for supervised learning problems.

\citeauthor{Isola.2017} develop Conditional Adversarial Networks (CANs)~\cite{Isola.2017} to tackle general mapping of one scene representation to another, e.~g. style transfer or colorization.
Using an L1 loss for low image frequencies and a patch-based discriminator for high frequencies, the net is able to learn a structured loss solely from the training data with no need for task specific loss engineering.

The architecture is based on DCGANs~\cite{Radford.2016} and the generator uses symmetric layer concatenation skip connections like a U-Net~\cite{Ronneberger.2015}.
We evaluate CANs on the GIANA polyp segmentation SD subtask and change the following:

Outputs are originally limited to a 256$\times$256 resolution, so we use bicubic upsampling at test time to achieve the desired output resolution.
The generated outputs are 3-channel images but very close to binary images, so we simply average all channels and threshold them by half the maximum value.

Training can easily be subject to vanishing gradients, so we determine the minimum batch size avoiding stagnation at the beginning (3) and optimum batch size (32, used for final results) as a compromise between learning stability and limited memory.

Dropout is applied both at training and test time to generate rich and naturalistic outputs but results for binary segmentation vary too much, therefore we deactivate dropout at test time.
The use of the dataset augmentation techniques in \cite{Vazquez.2017} worsens validation set performance by up to 2.8 percentage points so we do not use it in the final model.

\printbibliography

\end{document}
